# IOTSS Mini Porject - Gestures Classification

The mini project consists on get data for a 1 second window given a command and classify the gesture collects IMU data (gyro, accel, temperature) into a sliding window of samples and runs a TensorFlow Lite Micro model to recognize one of three gestures. It uses the QMI8658 driver for sensor access and the Eloquent TinyML wrapper + tflm_esp32 for running a TFLite Micro model on an ESP32.

### Required hardware
----

- ESP32 development board
- QMI8658 IMU
- Wires
- Computer with IDEs

### Required software/libraries
----

Install the following libraries into your Arduino IDE:

- DHT sensor library for ESPx (v1.19)
- FastIMU (v1.2.8)
- lvgl (v9.3.0)
- GFX Library for Arduino (v1.6.1)
- tflm_esp32 (v2.0.0)
- EloquentTinyML (v3.0.1)

Install the following Python libraries:

- tensorflow (2.18.0)
- numpy (1.26.0)
- sklearn
- everywhereml (>=0.2.32) *optional*

### Getting the Data
---

1. Place this sketch "task_1.ino" into the Arduino IDE.
2. Install the required libraries in the Arduino IDE.
3. Select the correct ESP32 S3 board and flash.
4. Run the pyhton program "task_2.py" to collect 30 repetitons of one gesture.
5. Delete file "rep_0.txt" generated by the previous step.
6. Save the 30 files into a directory.
7. Repeat steps 4 to 6 as many times as gestures you want to record.

### Training the Model
---

1. Open and run "task_3.ipynb" jupyter notebook. Follow the instructions inside.

### Building the Inference
----

1. Place this sketch "task_4.ino" into the Arduino IDE.
2. Add model_x.h (generated TFLite flatbuffer header)
3. Install the required libraries in the Arduino IDE.
4. Select the correct ESP32 S3 board and flash.
5. Open serial monitor to see inference results.


_Done by Marta Espejo & Jordi Nadeu_
