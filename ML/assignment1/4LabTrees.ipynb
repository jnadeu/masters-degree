{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsNNfCJ0bEcQ"
      },
      "source": [
        "# 4. Lab: Decision Trees, Random Forests and k-NN.\n",
        "## 1. Objectives\n",
        "\n",
        "\n",
        "1.  Understand the basics of decision trees, random forests and k-NN.\n",
        "2. Learn to work with simple datasets.\n",
        "3. Use decision trees for classification and regression.\n",
        "4. Use random forests and k-NN for classification.\n",
        "5. Experiment with different splitting criteria, pruning, number of estimators, v.\n",
        "6. Evaluate model performance using appropriate metrics.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## 2. Hand-in instructions\n",
        "You are expected to hand-in a Jupyter notebook based on this one. Your notebook should contain running code and answer all questions posed in this assignment.\n",
        "\n",
        "You can structure your code as you wish, as long as it is well structured and commented. You can structure it in different files or in a single one. The code provided in this assignment is a suggestion, feel free to solve the exercises as you wish\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9LNhnkBb3_h"
      },
      "source": [
        "# Decision Trees\n",
        "## Decision trees for classification\n",
        "First, we will implement a decision tree to classify Wine quality.\n",
        "Load the Wine Quality dataset from UCI Machine Learning Repository. Print information about the data like names of features, number of datapoints, data types.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mctovMbgcSyY"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_text, plot_tree\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "# Load the Wine Quality dataset\n",
        "# Downloading dataset from UCI Machine Learning Repository\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "data = pd.read_csv(url, delimiter=';')\n",
        "\n",
        "# Explore the dataset\n",
        "print(\"First 5 rows of the dataset:\\n\", data.head())\n",
        "print(\"\\nDataset Information:\")\n",
        "print(data.info())\n",
        "\n",
        "# Print feature names and basic statistics\n",
        "print(\"\\nFeature names:\", data.columns.tolist())\n",
        "print(\"\\nNumber of datapoints:\", data.shape[0])\n",
        "\n",
        "# Split the dataset into features (X) and target variable (y)\n",
        "X = data.drop('quality', axis=1)\n",
        "y = data['quality']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOTJmE7GjJ52"
      },
      "source": [
        "As we aim for a classification task, we will binarize the quality feature (high quality = a score of more than 5)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_dGSG-a8iWSy"
      },
      "outputs": [],
      "source": [
        "# Binarize the target variable for classification\n",
        "y_class = (y >= 6).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyucVMvKmj_9"
      },
      "source": [
        "Split the dataset into training and test sets using train_test_split from scikit learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDcv2aeRm3lm"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset into training and test sets for classification\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_class)\n",
        "\n",
        "# Print the shape of the train and test sets to confirm the split\n",
        "print(\"Training set shape (X_train):\", X_train.shape)\n",
        "print(\"Training set shape (y_train):\", y_train.shape)\n",
        "print(\"Test set shape (X_test):\", X_test.shape)\n",
        "print(\"Test set shape (y_test):\", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMaWC1Nnc0bC"
      },
      "source": [
        "Train a decision tree classifier and evaluate it using a confusion matrix and a classification report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYQAVw98cS8w"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\"\"\".venv/lib/python3.11/site-packages/sklearn/tree/_classes.py\n",
        "\n",
        "    random_state : int, RandomState instance or None, default=None\n",
        "        Controls the randomness of the estimator. The features are always\n",
        "        randomly permuted at each split, even if ``splitter`` is set to\n",
        "        ``\"best\"``. When ``max_features < n_features``, the algorithm will\n",
        "        select ``max_features`` at random at each split before finding the best\n",
        "        split among them.\n",
        "\"\"\"\n",
        "\n",
        "# Create and train the model\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = dt.predict(X_test)\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\n\")\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\n\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liHlNOSYcyob"
      },
      "source": [
        "**What is the random_state parameter?**\n",
        "\n",
        "The random_state is a parameter in scikit-learn's models, including the DecisionTreeClassifier, controls the randomness involved in certain operations.\n",
        "\n",
        "**What does changing it accomplish?**\n",
        "\n",
        "If random_state is changing, every time you train the model or split the data, it will produce different results due to the inherent randomness in the training process.\n",
        "\n",
        "**What is a confusion matrix? Analyze the confusion matrix that you obtained.**\n",
        "\n",
        "A confusion matrix is a performance measurement for classification tasks. \n",
        "If we look at the matrix we can see the diagonal representing the positive cases. In our case 136 and 148 with good prediction. On the other hand if we look at the top and bottom of the matrix we can know the number of cases in which the prediction has failed, which in our case is 60 and 56 that are less of the good ones.\n",
        "\n",
        "### Visualising decision trees\n",
        "We will explore different decision tree visualisations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3Pq7r5xesiu"
      },
      "outputs": [],
      "source": [
        "text_representation = export_text(dt)\n",
        "print(text_representation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVoUXciSfbON"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(25,20))\n",
        "_ = plot_tree(dt,\n",
        "                   feature_names=list(X.columns),\n",
        "                   class_names=None,\n",
        "                   filled=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChyWillLl1eY"
      },
      "source": [
        "**What do the graphs represent? Would you say a tree is explainable?**\n",
        "\n",
        "Represents a decision tree, likely generated after training a decision tree classifier on a dataset. Each node in the tree represents a decision based on one of the features, and the tree structure represents the recursive splitting of the dataset into subsets to make predictions (classification).\n",
        "\n",
        "---\n",
        "\n",
        "### Pruning decision trees\n",
        "\n",
        "We will now pre-prune the tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ihi_oIYDmc3v"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\"\"\" .venv/lib/python3.11/site-packages/sklearn/tree/_classes.py\n",
        "\n",
        "    max_depth : int, default=None\n",
        "        The maximum depth of the tree. If None, then nodes are expanded until\n",
        "        all leaves are pure or until all leaves contain less than\n",
        "        min_samples_split samples.\n",
        "\"\"\"\n",
        "\n",
        "# Pruning the tree\n",
        "dt_class_pruned = DecisionTreeClassifier(random_state=42, max_depth=4)\n",
        "dt_class_pruned.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate the pruned tree\n",
        "y_pred_class_pruned = dt_class_pruned.predict(X_test)\n",
        "print(\"Pruned Decision Tree Classification Accuracy:\", accuracy_score(y_test, y_pred_class_pruned))\n",
        "print(\"\\n\")\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_class_pruned))\n",
        "print(\"\\n\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_class_pruned))\n",
        "\n",
        "# Visualize the pruned tree\n",
        "print(export_text(dt_class_pruned, feature_names=list(X.columns)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKgbJnSonEK0"
      },
      "source": [
        "**What does the max_depth variable represent?**\n",
        "\n",
        "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
        "\n",
        "**By investigating different depths and confusion matrices, propose a depth for the tree that strikes a good balance between efficiency and accuracy.**\n",
        "\n",
        "My propose are max_depth=4 because we have an accuracy of 0.735. We can also use max_depth=6 and have a better accuracy but I think it's not relevant and are more efficient use 4.\n",
        "\n",
        "\n",
        "**What else could we change to achieve this balance?**\n",
        "\n",
        "To prevent overfitting, we can specify the minimum number of samples that are required to be at a leaf node or specify minimum number of samples that are required to split an internal node."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLiyeHd2xsxJ"
      },
      "source": [
        "### Splitting criteria\n",
        "\n",
        "We will now change the splitting criteria. By default, the sklearn function sets it to the gini coefficient. Change it to another criteria, such as minimal entropy.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrAsnthsyR_a"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\"\"\" .venv/lib/python3.11/site-packages/sklearn/tree/_classes.py\n",
        "\n",
        "    criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n",
        "        The function to measure the quality of a split. Supported criteria are\n",
        "        \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n",
        "        Shannon information gain, see :ref:`tree_mathematical_formulation`.\n",
        "\"\"\"\n",
        "\n",
        "# Changing splitting criteria for the tree\n",
        "dt_class_other = DecisionTreeClassifier(random_state=42, criterion=\"entropy\")\n",
        "dt_class_other.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate the new tree\n",
        "y_pred_class_entropy = dt_class_other.predict(X_test)\n",
        "print(\"Entropy Decision Tree Classification Accuracy:\", accuracy_score(y_test, y_pred_class_entropy))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_class_entropy))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_class_entropy))\n",
        "\n",
        "# Visualize the pruned tree\n",
        "print(export_text(dt_class_other, feature_names=list(X.columns)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spkhdVk_zIvd"
      },
      "source": [
        "**What differences do you observe between trees trained with different splitting criteria?**\n",
        "\n",
        "When you change the splitting criterion in a decision tree from the default Gini impurity to entropy, you are altering the way the tree selects the best splits at each node.\n",
        "\n",
        "In the case of Gini criteria, it measures how often a randomly chosen element from the set would be incorrectly classified if randomly labeled according to the distribution of labels in the subset. It focuses on minimizing misclassification.\n",
        "\n",
        "In the case of Entropy criteria, it measures the randomness in the data. Evaluates how much information is gained by splitting the dataset based on a certain feature.\n",
        "\n",
        "And for log loss as a splitting criterion can enhance the performance of decision trees in many contexts, especially when probabilistic interpretation is crucial. However, it is essential to balance the complexity and potential overfitting risks with the benefits gained from more accurate probability estimate. But I think it's really similar to entropy criteria.\n",
        "\n",
        "In summary, we can improve the efficiency and accuracy of the splitting process depending on the criteria we use and the nature of the dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o_wSiaAn7fD"
      },
      "source": [
        "## Decision trees for regression\n",
        "We will now train a decision tree for a regression task on the same dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKWpVGRjnNvG"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset into training and test sets for regression\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Using scikit-learn for Decision Trees (Regression)\n",
        "# Create and train the model\n",
        "dt_reg = DecisionTreeRegressor(random_state=42)\n",
        "dt_reg.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_reg = dt_reg.predict(X_test_reg)\n",
        "print(\"Decision Tree Regression Mean Squared Error:\", mean_squared_error(y_test_reg, y_pred_reg))\n",
        "print(\"Decision Tree Regression R^2 Score:\", r2_score(y_test_reg, y_pred_reg))\n",
        "\n",
        "# Prune the regression tree\n",
        "dt_reg_pruned = DecisionTreeRegressor(random_state=42, max_depth=5)\n",
        "dt_reg_pruned.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Predict and evaluate the pruned tree\n",
        "y_pred_reg_pruned = dt_reg_pruned.predict(X_test_reg)\n",
        "print(\"Pruned Decision Tree Regression Mean Squared Error:\", mean_squared_error(y_test_reg, y_pred_reg_pruned))\n",
        "print(\"Pruned Decision Tree Regression R^2 Score:\", r2_score(y_test_reg, y_pred_reg_pruned))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aV3FAzeom6X"
      },
      "source": [
        "**By investigating different depths and errors, propose a depth for the tree that strikes a good balance between efficiency and accuracy.**\n",
        "\n",
        "I think a max_depth=5 might strike a good balance between efficiency and accuracy.\n",
        "\n",
        "**Why would a pruned tree be advisable, rather than using the tree at its maximum depth?**\n",
        "\n",
        "Because a pruned tree focuses on the most critical splits, reducing the risk of overfitting and ensuring that the model generalizes better to unseen data.\n",
        "\n",
        "A pruned tree is typically more robust, interpretable, and computationally efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1ZljtAhp4lO"
      },
      "source": [
        "# Random Forests\n",
        "\n",
        "We will train a random forest model for the initial classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvDtz-kqqJeq"
      },
      "outputs": [],
      "source": [
        "# Create and train the model\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_class = rf.predict(X_test)\n",
        "print(\"Random Forest Classification Accuracy:\", accuracy_score(y_test, y_pred_class))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_class))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_class))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IEbJRjJqhXf"
      },
      "source": [
        "**Compare the classification report and confusion matrices of the single tree and of the forest. What do you observe?**\n",
        "\n",
        "The Random forests reduce the high variance that single decision trees typically exhibit. A single decision tree may overfit on certain training sets, which could lead to poor generalization on the test set. Random forests, by averaging the results of multiple trees, tend to provide more robust and generalizable results.\n",
        "\n",
        "While the improvement in accuracy might seem small, itâ€™s significant when considering the generalization ability. Random forests generally perform better because they aggregate the results from multiple decision trees, reducing the impact of overfitting that can occur with a single decision tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhBPuGJBuGDL"
      },
      "source": [
        "## Visualising random forests\n",
        "Let's check out individual trees of the forest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87S3tW6yuKh6"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(25,20))\n",
        "_=plot_tree(rf.estimators_[0], feature_names=list(X.columns),\n",
        "                   class_names=None,\n",
        "                   filled=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q84RwHnvEG4"
      },
      "source": [
        "**Check different base learners of the forest and compare them between them and to the single tree we trained at the beginning of the lab. What do you observe?**\n",
        "\n",
        "I see them quite similar, but the forest looks like it grows more horizontally than the single tree, maybe because in the random forest each base learner is essentially a decision tree trained on a random subset of the data and random subset of features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF6uTXN0wy7e"
      },
      "source": [
        "## Tuning random forests\n",
        "\n",
        "We will now analyse different parameters that we can tune in the random forest model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChJcqDQtw5Gc"
      },
      "outputs": [],
      "source": [
        "# Tuning Random Forest parameters\n",
        "rf_tuned = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
        "rf_tuned.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate the tuned Random Forest\n",
        "y_pred_class_tuned = rf_tuned.predict(X_test)\n",
        "print(\"Tuned Random Forest Classification Accuracy:\", accuracy_score(y_test, y_pred_class_tuned))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_class_tuned))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_class_tuned))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gY_A7LTxPDG"
      },
      "source": [
        "**Experiment with the different parameters. Propose an appropriate value for each parameter to maximise efficiency and accuracy.**\n",
        "\n",
        "For n_estimators parameter we can experiment with values between 100 to 300. We increase the number of trees to 250 but don't give a boost in accuracy, in fact we lost a little bit of precision. Beyond 300, we encounter diminishing returns.\n",
        "\n",
        "For max_depth parameter we try between 10 to 15 but we don't improve the accuracy and get worse efficiency. Also we try depth <10 we lose accuracy. \n",
        "\n",
        "In summary the best value are around to n_estimators=200 and max_depth=10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4HN3Sti0x8E"
      },
      "source": [
        "# K-NN\n",
        "\n",
        "We will perform the same tasks but with a k-NN model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ispwYLQ0-oi"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Using scikit-learn for kNN\n",
        "# Create and train the model\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_class = knn.predict(X_test)\n",
        "print(\"kNN Classification Accuracy:\", accuracy_score(y_test, y_pred_class))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_class))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_class))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT4Rk9X_2H3X"
      },
      "source": [
        "**Find an appropriate k to maximise accuracy while avoiding overfitting.**\n",
        "\n",
        "To optimize the k-NN model and find the best k value, we need to balance bias (underfitting) and variance (overfitting).\n",
        "\n",
        "I think the best k is around 5. We also can put a higher value like 11 but it not increase accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZgqCh6rjB8z"
      },
      "source": [
        "# Grading chart\n",
        "\n",
        "|Category |\tPoints |\tDescription|\n",
        "|---------|--------|-------------|\n",
        "|Code Functionality and report|\t3 |\tThe code runs without errors and all questions are answered in a readable manner. |\n",
        "|Data Loading and Exploration|\t1 |\tCorrectly explores the Wine Quality dataset |\n",
        "|Model Evaluation for Decision Trees|\t1.5|\tCorrectly calculates and displays accuracy, confusion matrix, and classification report for decision trees.|\n",
        "|Pruning and Analysis|\t1.5\t| Implements pruning correctly, evaluates and discusses the impact of tree depth on performance.|\n",
        "|Splitting Criteria|\t1|\tChanges splitting criteria and analyzes differences in performance (accuracy/confusion matrix).|\n",
        "|Random Forest Implementation|\t0.5\t| Correctly  evaluates performance with appropriate metrics.|\n",
        "|Visualization of Random Forests|\t0.5|\tVisualizes individual trees from the forest and provides a comparison with the decision tree.|\n",
        "|Tuning Random Forest Parameters|\t1 |\tDiscusses parameter tuning and proposes appropriate values for maximization of efficiency and accuracy.|\n",
        "|k-NN Implementation|\t1 |\tImplements k-NN and evaluates accuracy, confusion matrix, and classification report correctly.|\n",
        "|Finding Optimal k|\t1\t|Analyzes the choice of k in the k-NN model and discusses the balance between accuracy and overfitting.|\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
