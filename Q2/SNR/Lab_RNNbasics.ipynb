{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHsMhN4a6R0V"
      },
      "source": [
        "# Understanging RNNs\n",
        "\n",
        "# 1. Understanding the forward pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmsxqu-3w1h1"
      },
      "source": [
        "The first part of this lab is based on Andrew Ng's introduction to Machine Learning course. We start by importing all necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5QPzH1vu4si"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from scipy.special import softmax\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcgjhUBDfEEL"
      },
      "source": [
        "The goal of this part is to understand the forward pass in an RNN. We will build a function that implements a single forward step of an RNN cell, where the input is made-up of input data and of the output from the previous time step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiFe2dJgvOov"
      },
      "outputs": [],
      "source": [
        "def rnn_cell_forward(xt, ht, parameters):\n",
        "    \"\"\"\n",
        "    Implements a single forward step of an RNN-cell\n",
        "\n",
        "    Arguments:\n",
        "    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
        "    ht -- Output at timestep \"t-1\", numpy array of shape (n_a, m)\n",
        "    parameters -- dictionary containing:\n",
        "                        Wx -- Weight matrix for the input, numpy array of shape (n_a, n_x)\n",
        "                        Wh -- Weight matrix for the hidden state, numpy array of shape (n_a, n_a)\n",
        "                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                        ba --  Bias, numpy array of shape (n_a, 1)\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "    m enables processing multiple sequences simultaneously during training or inference, by inputing multiple inputs to be processed in parallel\n",
        "    Returns:\n",
        "    h_next -- next hidden state, of shape (n_a, m)\n",
        "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
        "    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve parameters from \"parameters\"\n",
        "    Wx = parameters[\"Wx\"]\n",
        "    Wh = parameters[\"Wh\"]\n",
        "    Wy = parameters[\"Wy\"]\n",
        "    ba = parameters[\"ba\"]\n",
        "    by = parameters[\"by\"]\n",
        "\n",
        "\n",
        "    # compute next activation state\n",
        "    h_next = np.tanh(np.dot(Wh,ht) + np.dot(Wx,xt) + ba)\n",
        "    # compute output of the current cell using the formula given above\n",
        "    yt_pred = softmax(np.dot(Wy,h_next) + by)\n",
        "\n",
        "    # store values you need for backward propagation in cache\n",
        "    cache = (h_next, ht, xt, parameters)\n",
        "\n",
        "    return h_next, yt_pred, cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATu4HCxffjVC"
      },
      "source": [
        "Test the function by providing an input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uUdE_QFwaeW"
      },
      "outputs": [],
      "source": [
        "#intitialise the parameters randomly to test a forward pass\n",
        "np.random.seed(1)\n",
        "xt = np.random.randn(3,10)\n",
        "ht = np.random.randn(5,10)\n",
        "Wh = np.random.randn(5,5)\n",
        "Wx = np.random.randn(5,3)\n",
        "Wy = np.random.randn(2,5)\n",
        "ba = np.random.randn(5,1)\n",
        "by = np.random.randn(2,1)\n",
        "parameters = {\"Wh\": Wh, \"Wx\": Wx, \"Wy\": Wy, \"ba\": ba, \"by\": by}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "759jJBPpxKxh"
      },
      "outputs": [],
      "source": [
        "#Test a forward pass of the RNN cell, with a single time step\n",
        "h_next, yt_pred, cache = rnn_cell_forward(xt, ht, parameters)\n",
        "print(\"h_next[4] = \", h_next[4])\n",
        "print(\"h_next.shape = \", h_next.shape)\n",
        "print(\"yt_pred[1] =\", yt_pred[1])\n",
        "print(\"yt_pred.shape = \", yt_pred.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7B13J-p1lIh"
      },
      "source": [
        "The RNN uses this forward pass repeatedly for the entirety of the sequence of inputs, starting from the first in the sequence at timestep 1 and continuing through the sequence.\n",
        "\n",
        "We will create a vector *h* to store all hidden states (that are provided as input to the next time-step), a vector *y* to store all predictions and a vector *caches* containing the list of caches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERkyVX8T2uyG"
      },
      "outputs": [],
      "source": [
        "def rnn_forward(x, h0, parameters):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation of the recurrent neural network described in Figure (3).\n",
        "\n",
        "    Arguments:\n",
        "    x -- Input data for every time-step, of shape (n_x, m, T_x).\n",
        "    h0 -- Initial hidden state, of shape (n_a, m)\n",
        "    parameters -- python dictionary containing:\n",
        "                        Wh -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
        "                        Wx -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
        "                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                        ba --  Bias numpy array of shape (n_a, 1)\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "\n",
        "    Returns:\n",
        "    h -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
        "    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
        "    caches -- tuple of values needed for the backward pass, contains (list of caches, x)\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize \"caches\" which will contain the list of all caches\n",
        "    caches = []\n",
        "\n",
        "    # Retrieve dimensions from shapes of x and parameters[\"Wya\"]\n",
        "    n_x, m, T_x = x.shape\n",
        "    n_y, n_a = parameters[\"Wy\"].shape\n",
        "\n",
        "\n",
        "    # initialize \"a\" and \"y\" with zeros\n",
        "    h = np.zeros([n_a,m,T_x])\n",
        "    y_pred = np.zeros([n_y,m,T_x])\n",
        "\n",
        "    # Initialize a_next\n",
        "    h_next = h0\n",
        "\n",
        "    # loop over all time-steps\n",
        "    for t in range(T_x):\n",
        "        # Update next hidden state, compute the prediction, get the cache (≈1 line)\n",
        "        h_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], h_next, parameters)\n",
        "        # Save the value of the new \"next\" hidden state in a (≈1 line)\n",
        "        h[:,:,t] = h_next\n",
        "        # Save the value of the prediction in y (≈1 line)\n",
        "        y_pred[:,:,t] = yt_pred\n",
        "        # Append \"cache\" to \"caches\" (≈1 line)\n",
        "        caches.append(cache)\n",
        "\n",
        "    # store values needed for backward propagation in cache\n",
        "    caches = (caches, x)\n",
        "\n",
        "    return h, y_pred, caches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsAGa3x4f7Ve"
      },
      "source": [
        "Test a forward pass of the model by providing an input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9Y0ANeS1kJT"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\n",
        "x = np.random.randn(3,10,4)\n",
        "h0 = np.random.randn(5,10)\n",
        "Wh = np.random.randn(5,5)\n",
        "Wx = np.random.randn(5,3)\n",
        "Wy = np.random.randn(2,5)\n",
        "ba = np.random.randn(5,1)\n",
        "by = np.random.randn(2,1)\n",
        "parameters = {\"Wh\": Wh, \"Wx\": Wx, \"Wy\": Wy, \"ba\": ba, \"by\": by}\n",
        "\n",
        "h, y_pred, caches = rnn_forward(x, h0, parameters)\n",
        "print(\"h[4][1] = \", h[4][1])\n",
        "print(\"h.shape = \", h.shape)\n",
        "print(\"y_pred[1][3] =\", y_pred[1][3])\n",
        "print(\"y_pred.shape = \", y_pred.shape)\n",
        "print(\"caches[1][1][3] =\", caches[1][1][3])\n",
        "print(\"len(caches) = \", len(caches))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hZeZTAAW2E1"
      },
      "source": [
        "Optional extension: create a function to calculate the gradient after a forward pass. Then, you can create a function to train your from-scratch RNN.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7sMxbYe7W0B"
      },
      "source": [
        "# 2. RNN training\n",
        "\n",
        "This part of the lab is based on the tutorial at https://machinelearningmastery.com/understanding-simple-recurrent-neural-networks-in-keras/ . Now we will use a library to build our RNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPPXoZze7skv"
      },
      "outputs": [],
      "source": [
        "def create_RNN(hidden_units, dense_units, input_shape, activation):\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n",
        "                        activation=activation[0]))\n",
        "    model.add(Dense(units=dense_units, activation=activation[1]))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "demo_model = create_RNN(2, 1, (3,1), activation=['linear', 'linear'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrr2_DMY8HQz"
      },
      "source": [
        "We will use a dataset containing the Monthly Mean Total Sunspot Number, from 1749/01/01 to 2017/08/31. This is a sequential dataset, containing the evolution of the number of susnpots in time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpUH72DO8qOn"
      },
      "source": [
        "We load the dataset, split it into test and train and scale it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qA8p5KMu87fX"
      },
      "outputs": [],
      "source": [
        "# Parameter split_percent defines the ratio of training examples\n",
        "def get_train_test(url, split_percent=0.8):\n",
        "    df = read_csv(url, usecols=[1], engine='python')\n",
        "    data = np.array(df.values.astype('float32'))\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    data = scaler.fit_transform(data).flatten()\n",
        "    n = len(data)\n",
        "    # Point for splitting data into train and test\n",
        "    split = int(n*split_percent)\n",
        "    train_data = data[range(split)]\n",
        "    test_data = data[split:]\n",
        "    return train_data, test_data, data\n",
        "\n",
        "sunspots_url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-sunspots.csv'\n",
        "train_data, test_data, data = get_train_test(sunspots_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilik2cZG9Jk5"
      },
      "source": [
        "The next step is to prepare the data for Keras model training. The input array should be shaped as: total_samples x time_steps x features.\n",
        "\n",
        "Timesteps indicates the number of timesteps that we will consider in each training sequence. In order to get these sequences for training, we’ll create input rows with non-overlapping time steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZDKV24s9Isw"
      },
      "outputs": [],
      "source": [
        "# Prepare the input X and target Y\n",
        "def get_XY(dat, time_steps):\n",
        "    # Indices of target array\n",
        "    Y_ind = np.arange(time_steps, len(dat), time_steps)\n",
        "    Y = dat[Y_ind]\n",
        "    # Prepare X\n",
        "    rows_x = len(Y)\n",
        "    X = dat[range(time_steps*rows_x)]\n",
        "    X = np.reshape(X, (rows_x, time_steps, 1))\n",
        "    return X, Y\n",
        "\n",
        "time_steps = 12\n",
        "trainX, trainY = get_XY(train_data, time_steps)\n",
        "testX, testY = get_XY(test_data, time_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYv0kb9Q9ypI"
      },
      "source": [
        "We can now train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfnH5O_P90K9"
      },
      "outputs": [],
      "source": [
        "model = create_RNN(hidden_units=3, dense_units=1, input_shape=(time_steps,1),\n",
        "                   activation=['tanh', 'tanh'])\n",
        "model.fit(trainX, trainY, epochs=20, batch_size=1, verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2b4MGUd_WDq"
      },
      "source": [
        "Note that in every epoch, 12 derivatives (one for each time step) are added to form the updating gradient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDWJb11b-Fvr"
      },
      "outputs": [],
      "source": [
        "def print_error(trainY, testY, train_predict, test_predict):\n",
        "    # Error of predictions\n",
        "    train_rmse = math.sqrt(mean_squared_error(trainY, train_predict))\n",
        "    test_rmse = math.sqrt(mean_squared_error(testY, test_predict))\n",
        "    # Print RMSE\n",
        "    print('Train RMSE: %.3f ' % (train_rmse))\n",
        "    print('Test RMSE: %.3f ' % (test_rmse))\n",
        "\n",
        "# make predictions\n",
        "train_predict = model.predict(trainX)\n",
        "test_predict = model.predict(testX)\n",
        "# Mean square error\n",
        "print_error(trainY, testY, train_predict, test_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTvjkQ1xhnOc"
      },
      "source": [
        "Exercise: To obtain the prediction of sunspots, reverse the scaling, for example with the  inverse_transform function. Check your predictions againt the dataset. You can also make predictions across many years, using your own predictions as inputs. You can compare the predictions with an updated list of sunspots here: https://www.sidc.be/SILSO/datafiles . For what length of time does the model maintain accuracy?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
